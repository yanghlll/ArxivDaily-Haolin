---
title: Latest 15 Papers - October 22, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Video Understanding
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding](http://arxiv.org/abs/2510.17305v2)** | 2025-10-21 | <details><summary>Submi...</summary><p>Submitted to ARR Rolling Review</p></details> |
| **[Think With Videos For Agentic Long-Video Understanding](http://arxiv.org/abs/2506.10821v5)** | 2025-10-21 |  |
| **[StreamingTOM: Streaming Token Compression for Efficient Video Understanding](http://arxiv.org/abs/2510.18269v1)** | 2025-10-21 |  |
| **[MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues](http://arxiv.org/abs/2510.17722v1)** | 2025-10-20 | <details><summary>Proje...</summary><p>Project Website: https://github.com/NJU-LINK/MT-Video-Bench</p></details> |
| **[ActAlign: Zero-Shot Fine-Grained Video Classification via Language-Guided Sequence Alignment](http://arxiv.org/abs/2506.22967v3)** | 2025-10-19 | <details><summary>Accep...</summary><p>Accepted to TMLR 2025 - Project page: https://amir-aghdam.github.io/act-align/</p></details> |
| **[Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features](http://arxiv.org/abs/2510.16781v1)** | 2025-10-19 |  |
| **[StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales](http://arxiv.org/abs/2510.16209v1)** | 2025-10-17 |  |
| **[Symmetric Entropy-Constrained Video Coding for Machines](http://arxiv.org/abs/2510.15347v1)** | 2025-10-17 | <details><summary>This ...</summary><p>This paper is prepared to submit to the IEEE Transactions</p></details> |
| **[SVAG-Bench: A Large-Scale Benchmark for Multi-Instance Spatio-temporal Video Action Grounding](http://arxiv.org/abs/2510.13016v2)** | 2025-10-16 |  |
| **[VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning](http://arxiv.org/abs/2510.14672v1)** | 2025-10-16 | <details><summary>Accep...</summary><p>Accepted by ICCV 2025</p></details> |
| **[Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding](http://arxiv.org/abs/2510.14032v1)** | 2025-10-15 | <details><summary>NeurI...</summary><p>NeurIPS 2025 (Spotlight). Webpage at https://xiaoqian-shen.github.io/Vgent</p></details> |
| **[InteractiveOmni: A Unified Omni-modal Model for Audio-Visual Multi-turn Dialogue](http://arxiv.org/abs/2510.13747v1)** | 2025-10-15 |  |
| **[Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning](http://arxiv.org/abs/2505.16836v3)** | 2025-10-15 | 34 pages, 25 figures |
| **[VideoLucy: Deep Memory Backtracking for Long Video Understanding](http://arxiv.org/abs/2510.12422v1)** | 2025-10-14 | <details><summary>NeurI...</summary><p>NeurIPS-2025 Accepted Paper</p></details> |
| **[K-frames: Scene-Driven Any-k Keyframe Selection for long video understanding](http://arxiv.org/abs/2510.13891v1)** | 2025-10-14 |  |

## World Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[VideoVerse: How Far is Your T2V Generator from a World Model?](http://arxiv.org/abs/2510.08398v2)** | 2025-10-21 | <details><summary>24 Pa...</summary><p>24 Pages, 8 Figures, 11 Tables</p></details> |
| **[Program Synthesis via Test-Time Transduction](http://arxiv.org/abs/2509.17393v3)** | 2025-10-21 | NeurIPS 2025 |
| **[Higher Embedding Dimension Creates a Stronger World Model for a Simple Sorting Task](http://arxiv.org/abs/2510.18315v1)** | 2025-10-21 |  |
| **[OmniNWM: Omniscient Driving Navigation World Models](http://arxiv.org/abs/2510.18313v1)** | 2025-10-21 | <details><summary>https...</summary><p>https://arlo0o.github.io/OmniNWM/</p></details> |
| **[SAMPO:Scale-wise Autoregression with Motion PrOmpt for generative world models](http://arxiv.org/abs/2509.15536v2)** | 2025-10-21 | 22 pages,15 figures |
| **[World-in-World: World Models in a Closed-Loop World](http://arxiv.org/abs/2510.18135v1)** | 2025-10-20 | <details><summary>Code ...</summary><p>Code is at https://github.com/World-In-World/world-in-world</p></details> |
| **[Can Image-To-Video Models Simulate Pedestrian Dynamics?](http://arxiv.org/abs/2510.17731v1)** | 2025-10-20 | <details><summary>Appea...</summary><p>Appeared in the ICML 2025 Workshop on Building Physically Plausible World Models, July 2025, https://physical-world-modeling.github.io/</p></details> |
| **[Morpheus: Benchmarking Physical Reasoning of Video Generative Models with Real Physical Experiments](http://arxiv.org/abs/2504.02918v2)** | 2025-10-20 |  |
| **[From Next Token Prediction to (STRIPS) World Models -- Preliminary Results](http://arxiv.org/abs/2509.13389v3)** | 2025-10-20 | 10 pages, 3 figures |
| **[SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries](http://arxiv.org/abs/2510.17482v1)** | 2025-10-20 | Under Review |
| **[General agents contain world models](http://arxiv.org/abs/2506.01622v5)** | 2025-10-20 | <details><summary>Accep...</summary><p>Accepted ICML 2025. Typos corrected</p></details> |
| **[DARIL: When Imitation Learning outperforms Reinforcement Learning in Surgical Action Planning](http://arxiv.org/abs/2507.05011v3)** | 2025-10-20 | <details><summary>Paper...</summary><p>Paper accepted at the MICCAI2025 workshop proceedings on COLlaborative Intelligence and Autonomy in Image-guided Surgery (COLAS)</p></details> |
| **[VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents](http://arxiv.org/abs/2510.16907v1)** | 2025-10-19 | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2025</p></details> |
| **[A Comprehensive Survey on World Models for Embodied AI](http://arxiv.org/abs/2510.16732v1)** | 2025-10-19 | <details><summary>https...</summary><p>https://github.com/Li-Zn-H/AwesomeWorldModels</p></details> |
| **[Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models](http://arxiv.org/abs/2510.16729v1)** | 2025-10-19 |  |

## Multimodal
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs](http://arxiv.org/abs/2510.18876v1)** | 2025-10-21 |  |
| **[AstroMMBench: A Benchmark for Evaluating Multimodal Large Language Models Capabilities in Astronomy](http://arxiv.org/abs/2510.00063v2)** | 2025-10-21 |  |
| **[Text Takes Over: A Study of Modality Bias in Multimodal Intent Detection](http://arxiv.org/abs/2508.16122v3)** | 2025-10-21 | <details><summary>EMNLP...</summary><p>EMNLP 2025 Main Conference Full Paper</p></details> |
| **[PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies](http://arxiv.org/abs/2510.16505v2)** | 2025-10-21 |  |
| **[CovMatch: Cross-Covariance Guided Multimodal Dataset Distillation with Trainable Text Encoder](http://arxiv.org/abs/2510.18583v1)** | 2025-10-21 | NeurIPS 2025 |
| **[A Multimodal Deep Learning Approach for White Matter Shape Prediction in Diffusion MRI Tractography](http://arxiv.org/abs/2504.18400v4)** | 2025-10-21 | <details><summary>Paper...</summary><p>Paper accepted to Human Brain Mapping. 25 pages, 3 figures, 8 tables</p></details> |
| **[Socialized Learning and Emergent Behaviors in Multi-Agent Systems based on Multimodal Large Language Models](http://arxiv.org/abs/2510.18515v1)** | 2025-10-21 |  |
| **[MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning](http://arxiv.org/abs/2510.08567v3)** | 2025-10-21 | <details><summary>We ha...</summary><p>We have come across a recent approach that has not been properly attributed at the time of submission and compared in a fair setting. Therefore, we would like to withdraw the paper to address these concerns</p></details> |
| **[StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal Decision-Making and Information Seeking](http://arxiv.org/abs/2510.18483v1)** | 2025-10-21 |  |
| **[DreamPRM-1.5: Unlocking the Potential of Each Instance for Multimodal Process Reward Model Training](http://arxiv.org/abs/2509.05542v2)** | 2025-10-21 |  |
| **[Med-2E3: A 2D-Enhanced 3D Medical Multimodal Large Language Model](http://arxiv.org/abs/2411.12783v2)** | 2025-10-21 |  |
| **[Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle](http://arxiv.org/abs/2508.05612v3)** | 2025-10-21 | <details><summary>Proje...</summary><p>Project page at: https://xenozlh.github.io/Shuffle-R1/</p></details> |
| **[Beyond Single Models: Mitigating Multimodal Hallucinations via Adaptive Token Ensemble Decoding](http://arxiv.org/abs/2510.18321v1)** | 2025-10-21 |  |
| **[The Impact of Image Resolution on Biomedical Multimodal Large Language Models](http://arxiv.org/abs/2510.18304v1)** | 2025-10-21 | <details><summary>Proce...</summary><p>Proceedings of the 10th Machine Learning for Healthcare Conference, PMLR 298, 2025</p></details> |
| **[Proactive Reasoning-with-Retrieval Framework for Medical Multimodal Large Language Models](http://arxiv.org/abs/2510.18303v1)** | 2025-10-21 | Work in progress |

## Multimodal LLM
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs](http://arxiv.org/abs/2510.18876v1)** | 2025-10-21 |  |
| **[See the Text: From Tokenization to Visual Reading](http://arxiv.org/abs/2510.18840v1)** | 2025-10-21 |  |
| **[From Objects to Anywhere: A Holistic Benchmark for Multi-level Visual Grounding in 3D Scenes](http://arxiv.org/abs/2506.04897v2)** | 2025-10-21 | <details><summary>NeurI...</summary><p>NeurIPS 2025 (Datasets and Benchmarks). Project page: https://anywhere-3d.github.io/</p></details> |
| **[Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs](http://arxiv.org/abs/2510.18279v1)** | 2025-10-21 | <details><summary>Accep...</summary><p>Accepted to EMNLP 2025 Findings. Previously titled "Text or Pixels? Evaluating Efficiency and Understanding of LLMs with Visual Text Inputs"</p></details> |
| **[MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues](http://arxiv.org/abs/2510.17722v1)** | 2025-10-20 | <details><summary>Proje...</summary><p>Project Website: https://github.com/NJU-LINK/MT-Video-Bench</p></details> |
| **[$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs](http://arxiv.org/abs/2510.17205v1)** | 2025-10-20 | EMNLP 2025 Main |
| **[Enrich and Detect: Video Temporal Grounding with Multimodal LLMs](http://arxiv.org/abs/2510.17023v1)** | 2025-10-19 | <details><summary>ICCV ...</summary><p>ICCV 2025 (Highlights)</p></details> |
| **[EEschematic: Multimodal-LLM Based AI Agent for Schematic Generation of Analog Circuit](http://arxiv.org/abs/2510.17002v1)** | 2025-10-19 |  |
| **[Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs](http://arxiv.org/abs/2510.16785v1)** | 2025-10-19 |  |
| **[VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs](http://arxiv.org/abs/2510.16598v1)** | 2025-10-18 | 22 pages, 8 figures |
| **[Scaling Beyond Context: A Survey of Multimodal Retrieval-Augmented Generation for Document Understanding](http://arxiv.org/abs/2510.15253v1)** | 2025-10-17 |  |
| **[Elevating Visual Perception in Multimodal LLMs with Visual Embedding Distillation](http://arxiv.org/abs/2412.09585v3)** | 2025-10-17 | <details><summary>Proje...</summary><p>Project Page: https://praeclarumjj3.github.io/visper_lm/</p></details> |
| **[Leveraging Multimodal LLM Descriptions of Activity for Explainable Semi-Supervised Video Anomaly Detection](http://arxiv.org/abs/2510.14896v1)** | 2025-10-16 |  |
| **[Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking](http://arxiv.org/abs/2510.14824v1)** | 2025-10-16 |  |
| **[Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image Perception, Transformation, and Reasoning](http://arxiv.org/abs/2510.12712v2)** | 2025-10-16 |  |

## Video Foundation Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[TTOM: Test-Time Optimization and Memorization for Compositional Video Generation](http://arxiv.org/abs/2510.07940v1)** | 2025-10-09 | <details><summary>Proje...</summary><p>Project page: https://ttom-t2v.github.io/</p></details> |
| **[Inferring Dynamic Physical Properties from Video Foundation Models](http://arxiv.org/abs/2510.02311v1)** | 2025-10-02 |  |
| **[Can World Models Benefit VLMs for World Dynamics?](http://arxiv.org/abs/2510.00855v1)** | 2025-10-01 | <details><summary>Proje...</summary><p>Project page: https://dyva-worldlm.github.io</p></details> |
| **[FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction](http://arxiv.org/abs/2509.21657v1)** | 2025-09-25 |  |
| **[Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls for Video Generation](http://arxiv.org/abs/2504.14899v2)** | 2025-09-20 | <details><summary>Proje...</summary><p>Project page: https://github.com/ewrfcas/Uni3C. Accepted by Siggraph Asian 2025</p></details> |
| **[Simplifying Traffic Anomaly Detection with Video Foundation Models](http://arxiv.org/abs/2507.09338v2)** | 2025-09-01 | <details><summary>ICCVW...</summary><p>ICCVW 2025 accepted. Code: https://github.com/tue-mps/simple-tad</p></details> |
| **[Autoregressive Universal Video Segmentation Model](http://arxiv.org/abs/2508.19242v1)** | 2025-08-26 |  |
| **[ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing](http://arxiv.org/abs/2508.10881v1)** | 2025-08-14 | <details><summary>Proje...</summary><p>Project Page: https://lg-li.github.io/project/tooncomposer</p></details> |
| **[SAGOnline: Segment Any Gaussians Online](http://arxiv.org/abs/2508.08219v1)** | 2025-08-11 | 19 pages, 10 figures |
| **[TRIBE: TRImodal Brain Encoder for whole-brain fMRI response prediction](http://arxiv.org/abs/2507.22229v1)** | 2025-07-29 |  |
| **[SAMITE: Position Prompted SAM2 with Calibrated Memory for Visual Object Tracking](http://arxiv.org/abs/2507.21732v1)** | 2025-07-29 |  |
| **[SeqTex: Generate Mesh Textures in Video Sequence](http://arxiv.org/abs/2507.04285v1)** | 2025-07-06 |  |
| **[SciVid: Cross-Domain Evaluation of Video Models in Scientific Applications](http://arxiv.org/abs/2507.03578v1)** | 2025-07-04 | <details><summary>ICCV ...</summary><p>ICCV 2025, GitHub repo: https://github.com/google-deepmind/scivid</p></details> |
| **[GenLit: Reformulating Single-Image Relighting as Video Generation](http://arxiv.org/abs/2412.11224v3)** | 2025-06-20 |  |
| **[SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis](http://arxiv.org/abs/2506.07603v2)** | 2025-06-16 |  |

