---
title: Latest 15 Papers - November 26, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Unified
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[iMontage: Unified, Versatile, Highly Dynamic Many-to-many Image Generation](https://arxiv.org/abs/2511.20635v1)** | 2025-11-25 |  |
| **[Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward](https://arxiv.org/abs/2511.20561v1)** | 2025-11-25 |  |
| **[HBridge: H-Shape Bridging of Heterogeneous Experts for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2511.20520v1)** | 2025-11-25 |  |
| **[A Unified Spatiotemporal Framework for Modeling Censored and Missing Areal Responses](https://arxiv.org/abs/2511.17725v2)** | 2025-11-25 |  |
| **[Unified Block Signal Processing Framework for LPWANs: Sequence Index Modulation Spreading](https://arxiv.org/abs/2511.20364v1)** | 2025-11-25 | <details><summary>13 pa...</summary><p>13 pages, 9 figures, 5 tables; submitted for possible publication</p></details> |
| **[Adversarial Robustness for Unified Multi-Modal Encoders via Efficient Calibration](https://arxiv.org/abs/2505.11895v2)** | 2025-11-25 |  |
| **[SafePR: Unified Approach for Safe Parallel Robots by Contact Detection and Reaction with Redundancy Resolution](https://arxiv.org/abs/2501.17773v3)** | 2025-11-25 |  |
| **[OmniAlpha: A Sequence-to-Sequence Framework for Unified Multi-Task RGBA Generation](https://arxiv.org/abs/2511.20211v1)** | 2025-11-25 |  |
| **[Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation](https://arxiv.org/abs/2510.24821v2)** | 2025-11-25 | 18 pages, 5 figures |
| **[Unified Text-Image-to-Video Generation: A Training-Free Approach to Flexible Visual Conditioning](https://arxiv.org/abs/2505.20629v2)** | 2025-11-25 | <details><summary>18 pa...</summary><p>18 pages, 10 figures, 8 tables</p></details> |
| **[ManipShield: A Unified Framework for Image Manipulation Detection, Localization and Explanation](https://arxiv.org/abs/2511.14259v2)** | 2025-11-25 |  |
| **[Face, Whole-Person, and Object Classification in a Unified Space Via The Interleaved Multi-Domain Identity Curriculum](https://arxiv.org/abs/2511.19846v1)** | 2025-11-25 |  |
| **[A Unified Evaluation-Instructed Framework for Query-Dependent Prompt Optimization](https://arxiv.org/abs/2511.19829v1)** | 2025-11-25 |  |
| **[UniGame: Turning a Unified Multimodal Model Into Its Own Adversary](https://arxiv.org/abs/2511.19413v1)** | 2025-11-24 |  |
| **[The Unified Non-Convex Framework for Robust Causal Inference: Overcoming the Gaussian Barrier and Optimization Fragility](https://arxiv.org/abs/2511.19284v1)** | 2025-11-24 | 10 pages, 1 table |

## Video Understanding
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Vision-Language Memory for Spatial Reasoning](https://arxiv.org/abs/2511.20644v1)** | 2025-11-25 |  |
| **[Multi-modal Generative AI: Multi-modal LLMs, Diffusions, and the Unification](https://arxiv.org/abs/2409.14993v3)** | 2025-11-25 | <details><summary>21 pa...</summary><p>21 pages, 10 figures, 3 tables</p></details> |
| **[Mistake Attribution: Fine-Grained Mistake Understanding in Egocentric Videos](https://arxiv.org/abs/2511.20525v1)** | 2025-11-25 | <details><summary>11 pa...</summary><p>11 pages, 4 figures, 6 tables</p></details> |
| **[Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier](https://arxiv.org/abs/2510.23506v3)** | 2025-11-25 | 16 pages, 11 figures |
| **[LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference](https://arxiv.org/abs/2510.11512v2)** | 2025-11-25 | 22 pages, 9 figures |
| **[VKnowU: Evaluating Visual Knowledge Understanding in Multimodal LLMs](https://arxiv.org/abs/2511.20272v1)** | 2025-11-25 | <details><summary>Data ...</summary><p>Data & Code: this https URL</p></details> |
| **[Alternating Perception-Reasoning for Hallucination-Resistant Video Understanding](https://arxiv.org/abs/2511.18463v2)** | 2025-11-25 | 32 pages, 36 figures |
| **[GHR-VQA: Graph-guided Hierarchical Relational Reasoning for Video Question Answering](https://arxiv.org/abs/2511.20201v1)** | 2025-11-25 |  |
| **[While recognizing actions, LMMs struggle to detect core interaction events](https://arxiv.org/abs/2511.20162v1)** | 2025-11-25 |  |
| **[A2Seek: Towards Reasoning-Centric Benchmark for Aerial Anomaly Understanding](https://arxiv.org/abs/2505.21962v2)** | 2025-11-25 |  |
| **[WaymoQA: A Multi-View Visual Question Answering Dataset for Safety-Critical Reasoning in Autonomous Driving](https://arxiv.org/abs/2511.20022v1)** | 2025-11-25 |  |
| **[Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click](https://arxiv.org/abs/2511.15948v2)** | 2025-11-25 |  |
| **[KEPT: Knowledge-Enhanced Prediction of Trajectories from Consecutive Driving Frames with Vision-Language Models](https://arxiv.org/abs/2509.02966v2)** | 2025-11-25 |  |
| **[CounterVQA: Evaluating and Improving Counterfactual Reasoning in Vision-Language Models for Video Understanding](https://arxiv.org/abs/2511.19923v1)** | 2025-11-25 |  |
| **[VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?](https://arxiv.org/abs/2411.10979v5)** | 2025-11-25 | <details><summary>Accep...</summary><p>Accepted to CVPR 2025</p></details> |

## World Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Cloud4D: Estimating Cloud Properties at a High Spatial and Temporal Resolution](https://arxiv.org/abs/2511.19431v2)** | 2025-11-25 | <details><summary>NeurI...</summary><p>NeurIPS 2025 Spotlight, project page: https://cloud4d.jacob-lin.com/</p></details> |
| **[LocateAnything3D: Vision-Language 3D Detection with Chain-of-Sight](https://arxiv.org/abs/2511.20648v1)** | 2025-11-25 | <details><summary>Tech ...</summary><p>Tech report. Project page: https://nvlabs.github.io/LocateAnything3D/</p></details> |
| **[Rethinking the Learning Paradigm for Facial Expression Recognition](https://arxiv.org/abs/2209.15402v3)** | 2025-11-25 |  |
| **[Unleashing the Power of Vision-Language Models for Long-Tailed Multi-Label Visual Recognition](https://arxiv.org/abs/2511.20641v1)** | 2025-11-25 |  |
| **[Planning in Branch-and-Bound: Model-Based Reinforcement Learning for Exact Combinatorial Optimization](https://arxiv.org/abs/2511.09219v3)** | 2025-11-25 |  |
| **[Reinforcing Action Policies by Prophesying](https://arxiv.org/abs/2511.20633v1)** | 2025-11-25 | <details><summary>https...</summary><p>https://LogosRoboticsGroup.github.io/ProphRL</p></details> |
| **[Wanderland: Geometrically Grounded Simulation for Open-World Embodied AI](https://arxiv.org/abs/2511.20620v1)** | 2025-11-25 |  |
| **[Discovering Spatial Patterns of Readmission Risk Using a Bayesian Competing Risks Model with Spatially Varying Coefficients](https://arxiv.org/abs/2511.20616v1)** | 2025-11-25 |  |
| **[Can Vibe Coding Beat Graduate CS Students? An LLM vs. Human Coding Tournament on Market-driven Strategic Planning](https://arxiv.org/abs/2511.20613v1)** | 2025-11-25 |  |
| **[Sparse-to-Field Reconstruction via Stochastic Neural Dynamic Mode Decomposition](https://arxiv.org/abs/2511.20612v1)** | 2025-11-25 |  |
| **[How to Purchase Labels? A Cost-Effective Approach Using Active Learning Markets](https://arxiv.org/abs/2511.20605v1)** | 2025-11-25 | <details><summary>Submi...</summary><p>Submitted as a preprint. 34 pages, 14 figures, 4 tables</p></details> |
| **[Towards Multimodal Graph Large Language Model](https://arxiv.org/abs/2506.09738v2)** | 2025-11-25 | 4 figures, 2 tables |
| **[BrowseSafe: Understanding and Preventing Prompt Injection Within AI Browser Agents](https://arxiv.org/abs/2511.20597v1)** | 2025-11-25 |  |
| **[PaTAS: A Parallel System for Trust Propagation in Neural Networks Using Subjective Logic](https://arxiv.org/abs/2511.20586v1)** | 2025-11-25 |  |
| **[MSTN: Fast and Efficient Multivariate Time Series Model](https://arxiv.org/abs/2511.20577v1)** | 2025-11-25 | <details><summary>21 pa...</summary><p>21 pages, 1 figure, 5 tables</p></details> |

## Multimodal
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)](https://arxiv.org/abs/2504.03151v2)** | 2025-11-25 |  |
| **[Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward](https://arxiv.org/abs/2511.20561v1)** | 2025-11-25 |  |
| **[New York Smells: A Large Multimodal Dataset for Olfaction](https://arxiv.org/abs/2511.20544v1)** | 2025-11-25 | <details><summary>Proje...</summary><p>Project website at https://smell.cs.columbia.edu</p></details> |
| **[ConceptGuard: Proactive Safety in Text-and-Image-to-Video Generation through Multimodal Risk Detection](https://arxiv.org/abs/2511.18780v2)** | 2025-11-25 |  |
| **[HBridge: H-Shape Bridging of Heterogeneous Experts for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2511.20520v1)** | 2025-11-25 |  |
| **[Adversarial Confusion Attack: Disrupting Multimodal Large Language Models](https://arxiv.org/abs/2511.20494v1)** | 2025-11-25 |  |
| **[MTBBench: A Multimodal Sequential Clinical Decision-Making Benchmark in Oncology](https://arxiv.org/abs/2511.20490v1)** | 2025-11-25 | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2025</p></details> |
| **[VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning](https://arxiv.org/abs/2511.20422v1)** | 2025-11-25 |  |
| **[Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier](https://arxiv.org/abs/2510.23506v3)** | 2025-11-25 | 16 pages, 11 figures |
| **[VKnowU: Evaluating Visual Knowledge Understanding in Multimodal LLMs](https://arxiv.org/abs/2511.20272v1)** | 2025-11-25 | <details><summary>Data ...</summary><p>Data & Code: this https URL</p></details> |
| **[HistoSpeckle-Net: Mutual Information-Guided Deep Learning for high-fidelity reconstruction of complex OrganAMNIST images via perturbed Multimode Fibers](https://arxiv.org/abs/2511.20245v1)** | 2025-11-25 |  |
| **[Decoupling and Damping: Structurally-Regularized Gradient Matching for Multimodal Graph Condensation](https://arxiv.org/abs/2511.20222v1)** | 2025-11-25 | <details><summary>11pag...</summary><p>11pages,5 figures,6 tables</p></details> |
| **[From Generation to Detection: A Multimodal Multi-Task Dataset for Benchmarking Health Misinformation](https://arxiv.org/abs/2505.18685v2)** | 2025-11-25 | <details><summary>Accep...</summary><p>Accepted to Findings of the Association for Computational Linguistics: EMNLP 2025</p></details> |
| **[Towards Benign Memory Forgetting for Selective Multimodal Large Language Model Unlearning](https://arxiv.org/abs/2511.20196v1)** | 2025-11-25 |  |
| **[FINE: Factorized multimodal sentiment analysis via mutual INformation Estimation](https://arxiv.org/abs/2511.20167v1)** | 2025-11-25 | <details><summary>15 pa...</summary><p>15 pages, 9 figures, conference</p></details> |

## Multimodal LLM
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)](https://arxiv.org/abs/2504.03151v2)** | 2025-11-25 |  |
| **[Beyond Generation: Multi-Hop Reasoning for Factual Accuracy in Vision-Language Models](https://arxiv.org/abs/2511.20531v1)** | 2025-11-25 | <details><summary>Accep...</summary><p>Accepted as poster at NewInML Workshop ICML, 2025</p></details> |
| **[HBridge: H-Shape Bridging of Heterogeneous Experts for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2511.20520v1)** | 2025-11-25 |  |
| **[MTBBench: A Multimodal Sequential Clinical Decision-Making Benchmark in Oncology](https://arxiv.org/abs/2511.20490v1)** | 2025-11-25 | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2025</p></details> |
| **[Emotion-Coherent Reasoning for Multimodal LLMs via Emotional Rationale Verifier](https://arxiv.org/abs/2510.23506v3)** | 2025-11-25 | 16 pages, 11 figures |
| **[Jailbreaking and Mitigation of Vulnerabilities in Large Language Models](https://arxiv.org/abs/2410.15236v3)** | 2025-11-25 |  |
| **[VKnowU: Evaluating Visual Knowledge Understanding in Multimodal LLMs](https://arxiv.org/abs/2511.20272v1)** | 2025-11-25 | <details><summary>Data ...</summary><p>Data & Code: this https URL</p></details> |
| **[Exploring the Synergy of Quantitative Factors and Newsflow Representations from Large Language Models for Stock Return Prediction](https://arxiv.org/abs/2510.15691v3)** | 2025-11-25 |  |
| **[VICoT-Agent: A Vision-Interleaved Chain-of-Thought Framework for Interpretable Multimodal Reasoning and Scalable Remote Sensing Analysis](https://arxiv.org/abs/2511.20085v1)** | 2025-11-25 |  |
| **[Metis-HOME: Hybrid Optimized Mixture-of-Experts for Multimodal Reasoning](https://arxiv.org/abs/2510.20519v2)** | 2025-11-25 |  |
| **[Large Language Models in Argument Mining: A Survey](https://arxiv.org/abs/2506.16383v6)** | 2025-11-25 | Work draft |
| **[Video Understanding with Large Language Models: A Survey](https://arxiv.org/abs/2312.17432v8)** | 2025-11-25 | <details><summary>Accep...</summary><p>Accepted to IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</p></details> |
| **[Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.10007v2)** | 2025-11-25 | <details><summary>This ...</summary><p>This paper has been accepted by AAAI-26</p></details> |
| **[4DWorldBench: A Comprehensive Evaluation Framework for 3D/4D World Generation Models](https://arxiv.org/abs/2511.19836v1)** | 2025-11-25 |  |
| **[SAFE: Harnessing LLM for Scenario-Driven ADS Testing from Multimodal Crash Data](https://arxiv.org/abs/2502.02025v2)** | 2025-11-24 | <details><summary>The p...</summary><p>The paper has been accepted for publication in the proceedings of the IEEE/ACM 48th International Conference on Software Engineering to be held 12-18 April 2026 (ICSE2026)</p></details> |

## Video Foundation Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Infinity-RoPE: Action-Controllable Infinite Video Generation Emerges From Autoregressive Self-Rollout](https://arxiv.org/abs/2511.20649v1)** | 2025-11-25 | <details><summary>Proje...</summary><p>Project Page: https://infinity-rope.github.io/</p></details> |
| **[Wanderland: Geometrically Grounded Simulation for Open-World Embodied AI](https://arxiv.org/abs/2511.20620v1)** | 2025-11-25 |  |
| **[Multi-modal Generative AI: Multi-modal LLMs, Diffusions, and the Unification](https://arxiv.org/abs/2409.14993v3)** | 2025-11-25 | <details><summary>21 pa...</summary><p>21 pages, 10 figures, 3 tables</p></details> |
| **[Adversarial Robustness for Unified Multi-Modal Encoders via Efficient Calibration](https://arxiv.org/abs/2505.11895v2)** | 2025-11-25 |  |
| **[Alternating Perception-Reasoning for Hallucination-Resistant Video Understanding](https://arxiv.org/abs/2511.18463v2)** | 2025-11-25 | 32 pages, 36 figures |
| **[ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment](https://arxiv.org/abs/2509.17818v2)** | 2025-11-25 | <details><summary>The p...</summary><p>The project page is at https://yychen233.github.io/ContextFlow-page</p></details> |
| **[Exo2EgoSyn: Unlocking Foundation Video Generation Models for Exocentric-to-Egocentric Video Synthesis](https://arxiv.org/abs/2511.20186v1)** | 2025-11-25 |  |
| **[Unified Text-Image-to-Video Generation: A Training-Free Approach to Flexible Visual Conditioning](https://arxiv.org/abs/2505.20629v2)** | 2025-11-25 | <details><summary>18 pa...</summary><p>18 pages, 10 figures, 8 tables</p></details> |
| **[Click2Graph: Interactive Panoptic Video Scene Graphs from a Single Click](https://arxiv.org/abs/2511.15948v2)** | 2025-11-25 |  |
| **[GigaBrain-0: A World Model-Powered Vision-Language-Action Model](https://arxiv.org/abs/2510.19430v2)** | 2025-11-25 | <details><summary>https...</summary><p>https://gigabrain0.github.io/</p></details> |
| **[SatSAM2: Motion-Constrained Video Object Tracking in Satellite Imagery using Promptable SAM2 and Kalman Priors](https://arxiv.org/abs/2511.18264v2)** | 2025-11-25 |  |
| **[GigaWorld-0: World Models as Data Engine to Empower Embodied AI](https://arxiv.org/abs/2511.19861v1)** | 2025-11-25 | <details><summary>Proje...</summary><p>Project Page: https://gigaworld0.github.io/</p></details> |
| **[HunyuanVideo 1.5 Technical Report](https://arxiv.org/abs/2511.18870v2)** | 2025-11-25 |  |
| **[4DWorldBench: A Comprehensive Evaluation Framework for 3D/4D World Generation Models](https://arxiv.org/abs/2511.19836v1)** | 2025-11-25 |  |
| **[The SA-FARI Dataset: Segment Anything in Footage of Animals for Recognition and Identification](https://arxiv.org/abs/2511.15622v2)** | 2025-11-24 |  |

