---
title: Latest 15 Papers - September 29, 2025
labels: documentation
---
**Please check the [Github](https://github.com/zezhishao/MTS_Daily_ArXiv) page for a better reading experience and more papers.**

## Video Understanding
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling](http://arxiv.org/abs/2412.05271v5)** | 2025-09-26 | Technical Report |
| **[Think With Videos For Agentic Long-Video Understanding](http://arxiv.org/abs/2506.10821v3)** | 2025-09-26 |  |
| **[VidBridge-R1: Bridging QA and Captioning for RL-based Video Understanding Models with Intermediate Proxy Tasks](http://arxiv.org/abs/2506.09079v2)** | 2025-09-26 |  |
| **[Prompt-guided Representation Disentanglement for Action Recognition](http://arxiv.org/abs/2509.21783v1)** | 2025-09-26 |  |
| **[VideoJudge: Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge for Video Understanding](http://arxiv.org/abs/2509.21451v1)** | 2025-09-25 | Work in progress |
| **[MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion Vectors](http://arxiv.org/abs/2509.17084v2)** | 2025-09-25 | 6 pages, 3 figures |
| **[Long Video Understanding with Learnable Retrieval in Video-Language Models](http://arxiv.org/abs/2312.04931v3)** | 2025-09-24 | <details><summary>Accep...</summary><p>Accepted by IEEE Transactions on Multimedia (TMM)</p></details> |
| **[COLT: Enhancing Video Large Language Models with Continual Tool Usage](http://arxiv.org/abs/2509.18754v2)** | 2025-09-24 | 16 pages |
| **[SurgVidLM: Towards Multi-grained Surgical Video Understanding with Large Language Model](http://arxiv.org/abs/2506.17873v2)** | 2025-09-24 |  |
| **[iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning](http://arxiv.org/abs/2509.19552v1)** | 2025-09-23 |  |
| **[ConViS-Bench: Estimating Video Similarity Through Semantic Concepts](http://arxiv.org/abs/2509.19245v1)** | 2025-09-23 | <details><summary>Accep...</summary><p>Accepted to NeurIPS 2025</p></details> |
| **[VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction](http://arxiv.org/abs/2509.19002v1)** | 2025-09-23 |  |
| **[Surgical Video Understanding with Label Interpolation](http://arxiv.org/abs/2509.18802v1)** | 2025-09-23 | 8 pages, 10 figures |
| **[AHA -- Predicting What Matters Next: Online Highlight Detection Without Looking Ahead](http://arxiv.org/abs/2509.16421v2)** | 2025-09-23 | <details><summary>Accep...</summary><p>Accepted at NeurIPS 2025, 32 pages, 5 figures</p></details> |
| **[STORM: Token-Efficient Long Video Understanding for Multimodal LLMs](http://arxiv.org/abs/2503.04130v4)** | 2025-09-22 |  |

## World Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[VLA-Reasoner: Empowering Vision-Language-Action Models with Reasoning via Online Monte Carlo Tree Search](http://arxiv.org/abs/2509.22643v1)** | 2025-09-26 | 9 pages |
| **[WoW: Towards a World omniscient World model Through Embodied Interaction](http://arxiv.org/abs/2509.22642v1)** | 2025-09-26 |  |
| **[Context and Diversity Matter: The Emergence of In-Context Learning in World Models](http://arxiv.org/abs/2509.22353v1)** | 2025-09-26 |  |
| **[Surgical Vision World Model](http://arxiv.org/abs/2503.02904v2)** | 2025-09-26 | <details><summary>This ...</summary><p>This paper has been accepted at the Data Engineering in Medical Imaging Workshop, MICCAI 2025</p></details> |
| **[Scalable In-Context Q-Learning](http://arxiv.org/abs/2506.01299v2)** | 2025-09-26 |  |
| **[Empowering Multi-Robot Cooperation via Sequential World Models](http://arxiv.org/abs/2509.13095v2)** | 2025-09-26 |  |
| **[MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation](http://arxiv.org/abs/2509.21797v1)** | 2025-09-26 | 11 pages, 4 figures |
| **[LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE](http://arxiv.org/abs/2509.21790v1)** | 2025-09-26 | 13 pages, 8 figures |
| **[Dream to Chat: Model-based Reinforcement Learning on Dialogues with User Belief Modeling](http://arxiv.org/abs/2508.16876v3)** | 2025-09-26 | <details><summary>Accep...</summary><p>Accepted to EMNLP 2025 Findings</p></details> |
| **[VDFD: Multi-Agent Value Decomposition Framework with Disentangled World Model](http://arxiv.org/abs/2309.04615v2)** | 2025-09-25 | 43 pages |
| **[FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction](http://arxiv.org/abs/2509.21657v1)** | 2025-09-25 |  |
| **[What Happens Next? Anticipating Future Motion by Generating Point Trajectories](http://arxiv.org/abs/2509.21592v1)** | 2025-09-25 |  |
| **[X-Streamer: Unified Human World Modeling with Audiovisual Interaction](http://arxiv.org/abs/2509.21574v1)** | 2025-09-25 | <details><summary>Proje...</summary><p>Project Page at https://byteaigc.github.io/X-Streamer</p></details> |
| **[ReSpace: Text-Driven 3D Indoor Scene Synthesis and Editing with Preference Alignment](http://arxiv.org/abs/2506.02459v3)** | 2025-09-25 | <details><summary>22 pa...</summary><p>22 pages, 17 figures (incl. appendix)</p></details> |
| **[From Next Token Prediction to (STRIPS) World Models -- Preliminary Results](http://arxiv.org/abs/2509.13389v2)** | 2025-09-25 | 10 pages, 3 figures |

## Multimodal
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs](http://arxiv.org/abs/2509.22646v1)** | 2025-09-26 | <details><summary>Proje...</summary><p>Project Page: https://deeptracereward.github.io/</p></details> |
| **[UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration](http://arxiv.org/abs/2509.22570v1)** | 2025-09-26 |  |
| **[Longitudinal and Multimodal Recording System to Capture Real-World Patient-Clinician Conversations for AI and Encounter Research: Protocol](http://arxiv.org/abs/2509.16378v2)** | 2025-09-26 | <details><summary>23 pa...</summary><p>23 pages, 2 figures, 2 tables</p></details> |
| **[Multimodal Recurrent Ensembles for Predicting Brain Responses to Naturalistic Movies (Algonauts 2025)](http://arxiv.org/abs/2507.17897v3)** | 2025-09-26 | <details><summary>8 pag...</summary><p>8 pages, 2 figures, 1 table. Invited report, CCN 2025 Algonauts Project session (3rd-place team). Code: https://github.com/erensemih/Algonauts2025_ModalityRNN v3: Added equal contribution footnote to author list</p></details> |
| **[Learning the Neighborhood: Contrast-Free Multimodal Self-Supervised Molecular Graph Pretraining](http://arxiv.org/abs/2509.22468v1)** | 2025-09-26 |  |
| **[GeoSketch: A Neural-Symbolic Approach to Geometric Multimodal Reasoning with Auxiliary Line Construction and Affine Transformation](http://arxiv.org/abs/2509.22460v1)** | 2025-09-26 |  |
| **[MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models](http://arxiv.org/abs/2506.17046v2)** | 2025-09-26 |  |
| **[Explaining multimodal LLMs via intra-modal token interactions](http://arxiv.org/abs/2509.22415v1)** | 2025-09-26 |  |
| **[Effectiveness of Large Multimodal Models in Detecting Disinformation: Experimental Results](http://arxiv.org/abs/2509.22377v1)** | 2025-09-26 | 9 pages |
| **[MMSearch-Plus: Benchmarking Provenance-Aware Search for Multimodal Browsing Agents](http://arxiv.org/abs/2508.21475v2)** | 2025-09-26 | <details><summary>Proje...</summary><p>Project Page: https://mmsearch-plus.github.io</p></details> |
| **[SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models](http://arxiv.org/abs/2504.04893v6)** | 2025-09-26 | <details><summary>Accep...</summary><p>Accepted at CVPR 2025 Workshop EVAL-FoMo-2</p></details> |
| **[StreetReaderAI: Making Street View Accessible Using Context-Aware Multimodal AI](http://arxiv.org/abs/2508.08524v4)** | 2025-09-26 | <details><summary>Accep...</summary><p>Accepted to UIST'25; v2. Fixed a missing word in the PDF; v3. Fixed a typo in an author's name; v4. Changed system name and title</p></details> |
| **[Aurora: Towards Universal Generative Multimodal Time Series Forecasting](http://arxiv.org/abs/2509.22295v1)** | 2025-09-26 |  |
| **[Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling](http://arxiv.org/abs/2412.05271v5)** | 2025-09-26 | Technical Report |
| **[InfiMed-Foundation: Pioneering Advanced Multimodal Medical Models with Compute-Efficient Pre-Training and Multi-Stage Fine-Tuning](http://arxiv.org/abs/2509.22261v1)** | 2025-09-26 |  |

## Multimodal LLM
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs](http://arxiv.org/abs/2509.22646v1)** | 2025-09-26 | <details><summary>Proje...</summary><p>Project Page: https://deeptracereward.github.io/</p></details> |
| **[Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning](http://arxiv.org/abs/2504.00907v3)** | 2025-09-26 |  |
| **[Explaining multimodal LLMs via intra-modal token interactions](http://arxiv.org/abs/2509.22415v1)** | 2025-09-26 |  |
| **[EmoPrefer: Can Large Language Models Understand Human Emotion Preferences?](http://arxiv.org/abs/2507.04278v4)** | 2025-09-26 |  |
| **[WAVE: Learning Unified & Versatile Audio-Visual Embeddings with Multimodal LLM](http://arxiv.org/abs/2509.21990v1)** | 2025-09-26 |  |
| **[SpectrumWorld: Artificial Intelligence Foundation for Spectroscopy](http://arxiv.org/abs/2508.01188v4)** | 2025-09-26 |  |
| **["She was useful, but a bit too optimistic": Augmenting Design with Interactive Virtual Personas](http://arxiv.org/abs/2508.19463v2)** | 2025-09-26 | <details><summary>The v...</summary><p>The version accepted for publication at International Journal of Human-Computer Studies</p></details> |
| **[VideoJudge: Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge for Video Understanding](http://arxiv.org/abs/2509.21451v1)** | 2025-09-25 | Work in progress |
| **[Semantic Edge-Cloud Communication for Real-Time Urban Traffic Surveillance with ViT and LLMs over Mobile Networks](http://arxiv.org/abs/2509.21259v1)** | 2025-09-25 | 17 pages, 12 figures |
| **[MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in Video Scenarios](http://arxiv.org/abs/2505.21333v2)** | 2025-09-25 | <details><summary>Accep...</summary><p>Accepted by NeurIPS 2025</p></details> |
| **[Text-Augmented Multimodal LLMs for Chemical Reaction Condition Recommendation](http://arxiv.org/abs/2407.15141v2)** | 2025-09-25 |  |
| **[Embodied AI: From LLMs to World Models](http://arxiv.org/abs/2509.20021v1)** | 2025-09-24 | <details><summary>Accep...</summary><p>Accepted by IEEE CASM</p></details> |
| **[MMSE-Calibrated Few-Shot Prompting for Alzheimer's Detection](http://arxiv.org/abs/2509.19926v1)** | 2025-09-24 |  |
| **[Adaptive Guidance Semantically Enhanced via Multimodal LLM for Edge-Cloud Object Detection](http://arxiv.org/abs/2509.19875v1)** | 2025-09-24 |  |
| **[Large Language Models for Pedestrian Safety: An Application to Predicting Driver Yielding Behavior at Unsignalized Intersections](http://arxiv.org/abs/2509.19657v1)** | 2025-09-24 |  |

## Video Foundation Model
| **Title** | **Date** | **Comment** |
| --- | --- | --- |
| **[FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction](http://arxiv.org/abs/2509.21657v1)** | 2025-09-25 |  |
| **[Uni3C: Unifying Precisely 3D-Enhanced Camera and Human Motion Controls for Video Generation](http://arxiv.org/abs/2504.14899v2)** | 2025-09-20 | <details><summary>Proje...</summary><p>Project page: https://github.com/ewrfcas/Uni3C. Accepted by Siggraph Asian 2025</p></details> |
| **[Simplifying Traffic Anomaly Detection with Video Foundation Models](http://arxiv.org/abs/2507.09338v2)** | 2025-09-01 | <details><summary>ICCVW...</summary><p>ICCVW 2025 accepted. Code: https://github.com/tue-mps/simple-tad</p></details> |
| **[Autoregressive Universal Video Segmentation Model](http://arxiv.org/abs/2508.19242v1)** | 2025-08-26 |  |
| **[ToonComposer: Streamlining Cartoon Production with Generative Post-Keyframing](http://arxiv.org/abs/2508.10881v1)** | 2025-08-14 | <details><summary>Proje...</summary><p>Project Page: https://lg-li.github.io/project/tooncomposer</p></details> |
| **[SAGOnline: Segment Any Gaussians Online](http://arxiv.org/abs/2508.08219v1)** | 2025-08-11 | 19 pages, 10 figures |
| **[TRIBE: TRImodal Brain Encoder for whole-brain fMRI response prediction](http://arxiv.org/abs/2507.22229v1)** | 2025-07-29 |  |
| **[SAMITE: Position Prompted SAM2 with Calibrated Memory for Visual Object Tracking](http://arxiv.org/abs/2507.21732v1)** | 2025-07-29 |  |
| **[Vidar: Embodied Video Diffusion Model for Generalist Bimanual Manipulation](http://arxiv.org/abs/2507.12898v2)** | 2025-07-27 |  |
| **[SeqTex: Generate Mesh Textures in Video Sequence](http://arxiv.org/abs/2507.04285v1)** | 2025-07-06 |  |
| **[SciVid: Cross-Domain Evaluation of Video Models in Scientific Applications](http://arxiv.org/abs/2507.03578v1)** | 2025-07-04 | <details><summary>ICCV ...</summary><p>ICCV 2025, GitHub repo: https://github.com/google-deepmind/scivid</p></details> |
| **[TriVLA: A Triple-System-Based Unified Vision-Language-Action Model for General Robot Control](http://arxiv.org/abs/2507.01424v2)** | 2025-07-03 |  |
| **[GenLit: Reformulating Single-Image Relighting as Video Generation](http://arxiv.org/abs/2412.11224v3)** | 2025-06-20 |  |
| **[SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis](http://arxiv.org/abs/2506.07603v2)** | 2025-06-16 |  |
| **[Seeing Voices: Generating A-Roll Video from Audio with Mirage](http://arxiv.org/abs/2506.08279v1)** | 2025-06-09 | <details><summary>Techn...</summary><p>Technical report website: mirage.app/research/seeing-voices, product website: mirage.app</p></details> |

